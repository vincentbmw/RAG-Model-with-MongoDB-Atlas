{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d80096-6553-4a76-aacd-7f4350f358aa",
   "metadata": {},
   "source": [
    "#  Populate MongoDB Atlas Database\n",
    "In this Python notebook, we will be generating embeddings of four dogs breed documents, indexing them and storing them into MongoDB Atlas database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca35cf58-20bb-4b5f-83c1-326707804cdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 1: The Documents\n",
    "\n",
    "These are the type of the dogs we'll be using. You can find these documents in the `data` folder:\n",
    "```text\n",
    "data/dogs\n",
    "├── alaskan-malmute.pdf\n",
    "└── american-bulldog.pdf\n",
    "└── golden-retriever.pdf\n",
    "└── siberian-husky.pdf\n",
    "└── border-collie.pdf\n",
    "└── german-shepherd.pdf\n",
    "└── rottweiler.pdf\n",
    "└── dalmation.pdf\n",
    "└── shiba-inu.pdf\n",
    "└── dobermann.pdf\n",
    "└── poodle.pdf\n",
    "└── chihuahua.pdf\n",
    "└── beagle.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c8c2c-6d1f-4140-9e89-3d2c33a7ef4f",
   "metadata": {},
   "source": [
    "## Step 2: Load Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b954d6-260a-4c63-8010-3ca1e036fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9fbec3-8c92-490d-a335-b237e1ab349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATLAS_URI Connection string found: mongodb+srv://cents29:FJ9BB0sUAXZQaZeh@cluster0.9jwfxrl.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\n"
     ]
    }
   ],
   "source": [
    "# Load settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# Change system path to root direcotry\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# For debugging purposes\n",
    "# print (config)\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "else:\n",
    "    print(\"ATLAS_URI Connection string found:\", ATLAS_URI)\n",
    "\n",
    "## Only uncomment this if you are using OpenAI for embeddings\n",
    "# OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
    "# if not OPENAI_API_KEY:\n",
    "#     raise Exception (\"'OPENAI_API_KEY' is not set. Please set it above to continue...\")\n",
    "# else:\n",
    "#     print(\"ATLAS_URI Connection string found:\", ATLAS_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0efbb5a-056d-47c5-b919-06a89367d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our variables\n",
    "DB_NAME = 'dogs'\n",
    "COLLECTION_NAME = 'type'\n",
    "INDEX_NAME = 'idx_embedding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efb3c69-d3c1-4e62-8d9f-a2043d3bec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex will download embeddings models as needed\n",
    "# Set llamaindex cache dir to ../cache dir here (Default is system tmp)\n",
    "# This way, we can easily see downloaded artifacts\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fdbca4f-d5af-4376-9c07-bd07c01c12d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(ATLAS_URI)\n",
    "\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a982758-25bf-4ea5-bc9f-ba7f4277ce1c",
   "metadata": {},
   "source": [
    "## Step 3: Setup Embeddings\n",
    "\n",
    "Now, we'll need to establish an embedding model to help us generate embeddings for the documents, so im using HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e6c48-655f-43a4-b9be-6b1030cc6fdf",
   "metadata": {},
   "source": [
    "### Using HuggingFace Embeddings\n",
    "\n",
    "This option utilizes a HuggingFace embedding model. Listed below are some examples, taken from the leaderboard https://huggingface.co/spaces/mteb/leaderboard. We'll be going with the `BAAI/bge-small-en-v.15` embedding model here.\n",
    "\n",
    "| model name                              | overall score | model size | model params | embedding length | License  | url                                                            |\n",
    "|-----------------------------------------|---------------|------------|--------------|------------------|----------|----------------------------------------------------------------|\n",
    "| BAAI/bge-large-en-v1.5                  | 64.x          | 1.34 GB    | 335 M        | 1024             | MIT      | https://huggingface.co/BAAI/bge-large-en-v1.5                  |\n",
    "| BAAI/bge-small-en-v1.5                  | 62.x          | 133 MB     | 33.5 M       | 384              | MIT      | https://huggingface.co/BAAI/bge-small-en-v1.5                  |\n",
    "| sentence-transformers/all-mpnet-base-v2 | 57.8          | 438 MB     |              | 768              | Apache 2 | https://huggingface.co/sentence-transformers/all-mpnet-base-v2 |\n",
    "| sentence-transformers/all-MiniLM-L12-v2 | 56.x          | 134 MB     |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 |\n",
    "| sentence-transformers/all-MiniLM-L6-v2  | 56.x          | 91 MB      |              | 384              | Apache 2 | https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422cdd99-b64f-4450-894a-1b020bf62d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# from llama_index.embeddings import HuggingFaceEmbedding\n",
    "# Uncomment the line above and comment the line below if you face an import error\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045a8bc6-7690-487f-ae0d-1c810b11be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_8672\\3880219279.py:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n"
     ]
    }
   ],
   "source": [
    "## Set up embedding model\n",
    "# The LLM used to generate natural language responses to queries\n",
    "# If not provided, it will default to gpt-3.5-turbo from OpenAI\n",
    "# If your OpenAI API key is not set, it will default to llama2-chat-13B from Llama.cpp\n",
    "# Since we don't need an LLM just yet, we'll be setting it to None\n",
    "\n",
    "# from llama_index import ServiceContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1822f497-13fd-4a7d-8969-d52440213a21",
   "metadata": {},
   "source": [
    "## Step 4: Connect Llama-Index and MongoDB Atlas\n",
    "\n",
    "i'll be using MongoDB Atlas as my vector storage. This is critical to store indexed data and then query later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90b09a80-cffb-4d7f-ba19-ea7e3d8285b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install llama-index-vector-stores-mongodb\n",
    "#!pip install llama-index-vector-stores-mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1ad3d42-9d62-4982-8440-0e5efa468e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = 'idx_embedding',\n",
    "                                 ## The following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c166f6-66d9-43bd-bd63-32801c43ac58",
   "metadata": {},
   "source": [
    "## Step 5: Read PDF Documents\n",
    "\n",
    "Llama-index has very handy `SimpleDirectoryReader` that can read single/multiple files and also an entire directory's content. I'll be using this to read my 10 PDF files and storing the data in `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3e4c0ae-4ec8-4e85-a984-088201fb2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 122 chunks from '../data/dogs/'\n",
      "CPU times: total: 2.38 s\n",
      "Wall time: 4.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# from llama_index.readers.file.base import SimpleDirectoryReader\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "data_dir = '../data/dogs/'\n",
    "\n",
    "## This reads an entire directory\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb54e893-f37a-4eff-bced-4d5eda3997ef",
   "metadata": {},
   "source": [
    "## Step 6: Index the Documents and Store Them Into MongoDB Atlas\n",
    "\n",
    "The code cell below is where everything that we've been preparing for in this comes together:\n",
    "- Embeddings are generated using our packaged-up embedding model `service_context` \n",
    "- Our documents `docs` get indexed `storage_context` - both text and embeddings are stored in MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfaaa754-b05a-41ce-8c79-a7ca0db033f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.55s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.95s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.40s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.07s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  8.00s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.02s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.00s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.34s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.55s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.47s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.93s/it]\n",
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 55s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e40d1-d76b-4abd-9e3b-f3aa65d60167",
   "metadata": {},
   "source": [
    "After running the code cell above, there should be a new database `dogs` and a new collection `type` inside it that contains the text as well as the generated embeddings of my 10 PDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada1a2e-30a3-434e-9abc-c87a49e052e3",
   "metadata": {},
   "source": [
    "#  Making Queries to the RAG Model\n",
    "In this section, we will be making use of our RAG model as well as an LLM to ask questions regarding our uploaded documents. If all goes to plan, our RAG model (powered by Atlas Vector Search) should be able to retrieve the portions of the document that's relevant to our query and feed that information to the LLM, thus enabling it to correctly answer our query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfe847-6b70-4ce3-bd73-c5aa7af85232",
   "metadata": {},
   "source": [
    "## Step 1: Setup LLM\n",
    "I'll need to setup an LLM to be able to take the results from the Atlas Vector Search and respond to the user query. We'll be using OpenAI again for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d01c22-529c-4828-a9a7-bc8306ddd626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-llama-api in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.11.post1 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-llms-llama-api) (0.10.34)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.6 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-llms-llama-api) (0.1.16)\n",
      "Requirement already satisfied: llamaapi<0.2.0,>=0.1.36 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-llms-llama-api) (0.1.36)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2024.3.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.1.19)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (3.3)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.25.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (10.3.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (4.11.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.9.4)\n",
      "Requirement already satisfied: pydantic>=1.10 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.7.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2024.4.28)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (3.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2024.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (2.18.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vince\\anaconda3\\envs\\atlas-1\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.11.post1->llama-index-llms-llama-api) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to install llama-index-llms-llama-api\n",
    "!pip install llama-index-llms-llama-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7309d7fd-6faa-446f-9fb2-612e3b615cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a well-known entrepreneur, investor, and essayist, best recognized as the co-founder of Y Combinator, a prominent startup accelerator.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_api import LlamaAPI\n",
    "\n",
    "api_key = config.get('LLAMA_API_KEY')\n",
    "llm = LlamaAPI(api_key=api_key)\n",
    "\n",
    "resp = llm.complete(\"Paul Graham is \")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7724a1-418f-4aeb-b06c-5438936eda8a",
   "metadata": {},
   "source": [
    "Awesome! Now that we've initialized both our embedding model as well as our LLM, let's combine them together into a unified interface `service_context` that we can use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93abd14-222a-43d4-9b60-060941900cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vince\\AppData\\Local\\Temp\\ipykernel_8672\\2502982726.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "# from llama_index import ServiceContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03345a-8b3a-4bc8-8176-770cc48c963c",
   "metadata": {},
   "source": [
    "## Step 4: Connect Llama-Index and MongoDB Atlas\n",
    "\n",
    "This is where everything comes together, we orchestrate the combination of MongoDB Atlas as our vector storage and the `service_context` we just defined. This system we've just set up will allow us to ask the LLM questions regarding our uploaded documents; Atlas Vector Search will then locate portions of the document that most closely matches our query to supplement the LLM's response, thereby providing us with a more accurate response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04dab438-1d2b-47e3-9655-a7a634d28f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# from llama_index.indices.vector_store.base import VectorStoreIndex\n",
    "# Uncomment the line above and comment away the line below if you face an import error\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = 'idx_embedding',\n",
    "                                 ## the following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5ca4f-5a08-4871-a951-f544e8b3569f",
   "metadata": {},
   "source": [
    "## Step 5: Query Data / Ask Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18aa5197-9bba-4dfc-972a-24d8dcc14e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 43.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>German Shepherds make some of the best canine partners and companions because they are truly invested in working with their humans and can excel at almost any sport or occupation when given the training and opportunity.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "response = index.as_query_engine().query(\"why german shepherd are the best partners? and not border collie\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-1",
   "language": "python",
   "name": "atlas-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
